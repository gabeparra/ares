# Glup Configuration
# Copy this file to .env and adjust settings

# LLM Provider (openai, grok, gemini, local)
# For RTX 4090, use 'local' with Ollama for best GPU performance
LLM_PROVIDER=local

# OpenAI (if using OpenAI)
OPENAI_API_KEY=

# Grok (if using Grok)
GROK_API_KEY=

# Gemini (if using Gemini)
GEMINI_API_KEY=

# Local Ollama Configuration (Recommended for RTX 4090)
# Ollama automatically detects and uses NVIDIA GPUs
OLLAMA_BASE_URL=http://localhost:11434

# Recommended models for RTX 4090 (24GB VRAM):
# - llama3.1:70b (best quality, requires ~40GB but works with quantization)
# - llama3:70b (excellent quality)
# - llama3.2:3b (fastest, good for testing)
# - mistral-nemo:12b (balanced)
# - qwen2.5:72b (alternative high-quality model)
# - mixtral:8x22b (great for complex reasoning)
OLLAMA_MODEL=llama3.1:70b

# Storage path (optional)
STORAGE_PATH=~/.caption_ai/segments.db
